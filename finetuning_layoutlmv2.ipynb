{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine-tuning LayoutLMv2 \n",
    "\n",
    "### Task DocVQA \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-18T09:01:37.065987Z",
     "start_time": "2023-07-18T09:01:37.031125Z"
    },
    "gather": {
     "logged": 1691402336759
    }
   },
   "outputs": [],
   "source": [
    "#import libraries\n",
    "\n",
    "from huggingface_hub import notebook_login\n",
    "from tqdm import tqdm\n",
    "from datasets import load_dataset, Dataset, Features, Sequence, Value, Array2D, Array3D, load_metric\n",
    "import pyarrow as pa\n",
    "import pyarrow.dataset as ds\n",
    "import pandas as pd\n",
    "import editdistance\n",
    "import numpy as np\n",
    "import transformers\n",
    "from transformers import AutoProcessor, AutoTokenizer, LayoutLMv2Processor, AutoModelForDocumentQuestionAnswering, TrainingArguments, Trainer, DefaultDataCollator\n",
    "\n",
    "from PIL import Image\n",
    "import os\n",
    "import torch\n",
    "import evaluate\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "gather": {
     "logged": 1691402340539
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f9dfbe8c489447dbb0899ec8a527622b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.svâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-06T13:17:35.643519Z",
     "start_time": "2023-07-06T13:17:35.639289Z"
    },
    "gather": {
     "logged": 1691402644577
    }
   },
   "outputs": [],
   "source": [
    "model_checkpoint = \"microsoft/layoutlmv2-base-uncased\"\n",
    "\n",
    "batch_size = 1"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-06T13:17:38.423205Z",
     "start_time": "2023-07-06T13:17:36.243805Z"
    },
    "gather": {
     "logged": 1691402648763
    }
   },
   "outputs": [],
   "source": [
    "dataset = load_dataset(\"svenjars/dataset\", cache_dir=\"/Users/svenja\")\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-06T13:17:38.429131Z",
     "start_time": "2023-07-06T13:17:38.423464Z"
    },
    "gather": {
     "logged": 1691402649058
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': Value(dtype='string', id=None),\n",
       " 'image': Value(dtype='string', id=None),\n",
       " 'query': Value(dtype='string', id=None),\n",
       " 'answer': Value(dtype='string', id=None),\n",
       " 'words': Sequence(feature=Value(dtype='string', id=None), length=-1, id=None),\n",
       " 'bounding_boxes': Sequence(feature=Sequence(feature=Value(dtype='int64', id=None), length=-1, id=None), length=-1, id=None)}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[\"train\"].features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "gather": {
     "logged": 1691402649377
    }
   },
   "outputs": [],
   "source": [
    "df_train = pd.DataFrame(dataset['train'])\n",
    "df_test = pd.DataFrame(dataset['test'])\n",
    "\n",
    "df_train['image'] = df_train['image'].apply(lambda x: 'training_data/images/' + x)\n",
    "df_test['image'] = df_test['image'].apply(lambda x: 'testing_data/images/' + x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "gather": {
     "logged": 1691402649662
    }
   },
   "outputs": [],
   "source": [
    "hg_dataset_train = Dataset(pa.Table.from_pandas(df_train))\n",
    "hg_dataset_test = Dataset(pa.Table.from_pandas(df_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.DataFrame(df_train)\n",
    "df_train.head()\n",
    "df_train.to_csv('LayoutLMv2_training_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = pd.DataFrame(df_test)\n",
    "df_test.head()\n",
    "df_test.to_csv('LayoutLMv2_testing_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "gather": {
     "logged": 1691402656993
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/20 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/10 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "updated_dataset_train = hg_dataset_train.map(\n",
    "    lambda example: {\"question\": example[\"query\"]},\n",
    "    remove_columns=[\"query\"]\n",
    ")\n",
    "\n",
    "\n",
    "updated_dataset_test = hg_dataset_test.map(\n",
    "    lambda example: {\"question\": example[\"query\"]},\n",
    "    remove_columns=[\"query\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "gather": {
     "logged": 1691402657705
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/20 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/20 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "updated_dataset_train = updated_dataset_train.filter(lambda x: len(x[\"words\"]) + len(x[\"question\"].split()) < 512)\n",
    "updated_dataset_train = updated_dataset_train.filter(lambda x: len(x[\"words\"]) + len(x[\"question\"].split()) < 512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "gather": {
     "logged": 1691402657995
    }
   },
   "outputs": [],
   "source": [
    "updated_dataset_train = updated_dataset_train.remove_columns(\"words\")\n",
    "updated_dataset_train = updated_dataset_train.remove_columns(\"bounding_boxes\")\n",
    "\n",
    "updated_dataset_test = updated_dataset_test.remove_columns(\"words\")\n",
    "updated_dataset_test = updated_dataset_test.remove_columns(\"bounding_boxes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "gather": {
     "logged": 1691402658896
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'training_data/images/data27.jpg'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "updated_dataset_train[11][\"image\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-06T13:29:46.887918Z",
     "start_time": "2023-07-06T13:29:44.533069Z"
    },
    "gather": {
     "logged": 1691402659530
    }
   },
   "outputs": [],
   "source": [
    "processor = AutoProcessor.from_pretrained(model_checkpoint)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing the document images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "gather": {
     "logged": 1691402659807
    }
   },
   "outputs": [],
   "source": [
    "image_processor = processor.image_processor\n",
    "\n",
    "root_dir = '/Users/svenja/Downloads/dataset/'\n",
    "\n",
    "#get words and boxes by using optical character recognition\n",
    "def get_ocr_words_and_boxes(examples):\n",
    "    \n",
    "    #get a batch of document images\n",
    "    images = [Image.open(root_dir + image_file).convert(\"RGB\") for image_file in examples['image']]\n",
    "    #resize every image to 224x224 + apply tesseract to get words + normalized boxes\n",
    "    examples['source'] = examples['image']\n",
    "    \n",
    "    encoded_inputs = image_processor(images)\n",
    "\n",
    "    examples['image'] = encoded_inputs.pixel_values\n",
    "    examples['words'] = encoded_inputs.words\n",
    "    examples['bounding_boxes'] = encoded_inputs.boxes\n",
    "\n",
    "    return examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "gather": {
     "logged": 1691402690881
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/20 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#applying the function to the dataset\n",
    "\n",
    "dataset_with_ocr_train = updated_dataset_train.map(get_ocr_words_and_boxes, batched=True, batch_size = 2)\n",
    "\n",
    "dataset_with_ocr_test = updated_dataset_test.map(get_ocr_words_and_boxes, batched=True, batch_size = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1691402708178
    }
   },
   "outputs": [],
   "source": [
    "print(dataset_with_ocr_train[0]['words'])\n",
    "print(dataset_with_ocr_train[0]['bounding_boxes'])\n",
    "print(\"-----\")\n",
    "print(dataset_with_ocr_train[1]['words'])\n",
    "print(dataset_with_ocr_train[1]['bounding_boxes'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "gather": {
     "logged": 1691402708633
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['id', 'image', 'answer', 'question', 'source', 'words', 'bounding_boxes'])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_with_ocr_train[0].keys()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing the text data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-06T14:04:36.411728Z",
     "start_time": "2023-07-06T14:04:36.406516Z"
    },
    "gather": {
     "logged": 1691402709368
    }
   },
   "outputs": [],
   "source": [
    "tokenizer = processor.tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-06T14:04:55.561983Z",
     "start_time": "2023-07-06T14:04:55.545756Z"
    },
    "gather": {
     "logged": 1691402709694
    }
   },
   "outputs": [],
   "source": [
    "def subfinder(words_list, answer_list):\n",
    "    matches = []\n",
    "    start_indices = []\n",
    "    end_indices = []\n",
    "    for idx, i in enumerate(range(len(words_list))):\n",
    "        if words_list[i] == answer_list[0] and words_list[i : i + len(answer_list)] == answer_list:\n",
    "            matches.append(answer_list)\n",
    "            start_indices.append(idx)\n",
    "            end_indices.append(idx + len(answer_list) - 1)\n",
    "    if matches:\n",
    "        return matches[0], start_indices[0], end_indices[0]\n",
    "    else:\n",
    "        return None, 0, 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "gather": {
     "logged": 1691402710042
    }
   },
   "outputs": [],
   "source": [
    "#example\n",
    "question = \"where is it located?\"\n",
    "words = [\"this\", \"is\", \"located\", \"in\", \"the\", \"university\", \"of\", \"california\", \"in\", \"the\", \"US\"]\n",
    "boxes = [[1000,1000,1000,1000] for _ in range(len(words))]\n",
    "answer = \"university of california\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "gather": {
     "logged": 1691402710592
    }
   },
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "gather": {
     "logged": 1691402710914
    }
   },
   "outputs": [],
   "source": [
    "assert isinstance(tokenizer, transformers.PreTrainedTokenizerFast)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "gather": {
     "logged": 1691402711228
    }
   },
   "outputs": [],
   "source": [
    "encoding = tokenizer(question, words, boxes=boxes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "gather": {
     "logged": 1691402711570
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[CLS] where is it located? [SEP] this is located in the university of california in the us [SEP]'"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(encoding.input_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "gather": {
     "logged": 1691402711939
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[None, 0, 1, 2, 3, 3, None, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, None]\n"
     ]
    }
   ],
   "source": [
    "print(encoding.word_ids())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "gather": {
     "logged": 1691402712249
    }
   },
   "outputs": [],
   "source": [
    "match, word_idx_start, word_idx_end = subfinder(words, answer.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "gather": {
     "logged": 1691402712583
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Match: ['university', 'of', 'california']\n",
      "Word idx start: 5\n",
      "Word idx end: 7\n"
     ]
    }
   ],
   "source": [
    "print(\"Match:\", match)\n",
    "print(\"Word idx start:\", word_idx_start)\n",
    "print(\"Word idx end:\", word_idx_end)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "gather": {
     "logged": 1691402712909
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token start index: 7\n",
      "Token end index: 17\n",
      "this is located in the university of california in the us\n",
      "Word ids: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
      "12\n",
      "14\n",
      "Reconstructed answer: university of california\n"
     ]
    }
   ],
   "source": [
    "sequence_ids = encoding.sequence_ids()\n",
    "\n",
    "#start token index of the current span in the text\n",
    "token_start_index = 0\n",
    "while sequence_ids[token_start_index] != 1:\n",
    "    token_start_index += 1\n",
    "\n",
    "#end token index of the current span in the text\n",
    "token_end_index = len(encoding.input_ids) - 1\n",
    "while sequence_ids[token_end_index] != 1:\n",
    "    token_end_index -= 1\n",
    "\n",
    "print(\"Token start index:\", token_start_index)\n",
    "print(\"Token end index:\", token_end_index)\n",
    "print(tokenizer.decode(encoding.input_ids[token_start_index:token_end_index+1]))\n",
    "\n",
    "word_ids = encoding.word_ids()[token_start_index:token_end_index+1]\n",
    "print(\"Word ids:\", word_ids)\n",
    "for id in word_ids:\n",
    "    if id == word_idx_start:\n",
    "        start_position = token_start_index \n",
    "    else:\n",
    "        token_start_index += 1\n",
    "\n",
    "for id in word_ids[::-1]:\n",
    "    if id == word_idx_end:\n",
    "        end_position = token_end_index \n",
    "    else:\n",
    "        token_end_index -= 1\n",
    "\n",
    "print(start_position)\n",
    "print(end_position)\n",
    "print(\"Reconstructed answer:\", tokenizer.decode(encoding.input_ids[start_position:end_position+1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-06T14:05:18.229094Z",
     "start_time": "2023-07-06T14:05:18.214433Z"
    },
    "gather": {
     "logged": 1691402713985
    }
   },
   "outputs": [],
   "source": [
    "#encode the data\n",
    "\n",
    "def encode_dataset(examples, max_length=512):\n",
    "    questions = examples[\"question\"]\n",
    "    words = examples[\"words\"]\n",
    "    boxes = examples[\"bounding_boxes\"]\n",
    "    answers = examples[\"answer\"]\n",
    "    sources = examples[\"source\"]\n",
    "\n",
    "    #encode the batch of examples and initialize the start_positions and end_positions\n",
    "    encoding = tokenizer(questions, words, boxes, max_length=max_length, padding=\"max_length\", truncation=True)\n",
    "    start_positions = []\n",
    "    end_positions = []\n",
    "\n",
    "    #loop through the examples in the batch\n",
    "    for i in range(len(questions)):\n",
    "        cls_index = encoding[\"input_ids\"][i].index(tokenizer.cls_token_id)\n",
    "\n",
    "        #find position of the answer in example's words\n",
    "        words_example = [word.lower() for word in words[i]]\n",
    "        answer = answers[i]\n",
    "        match, word_idx_start, word_idx_end = subfinder(words_example, answer.lower().split())\n",
    "\n",
    "        print(f\"{i}. match: {match} in {sources[i]}\")\n",
    "        print(f'\\nwords_example: {\" \".join(words_example)} \\nanswer: {answer}\\nquestion: {questions[i]}')\n",
    "\n",
    "        if match:\n",
    "            #if match is found, use token_type_ids to find where words start in the encoding\n",
    "            token_type_ids = encoding[\"token_type_ids\"][i]\n",
    "            token_start_index = 0\n",
    "            while token_type_ids[token_start_index] != 1:\n",
    "                token_start_index += 1\n",
    "\n",
    "            token_end_index = len(encoding[\"input_ids\"][i]) - 1\n",
    "            while token_type_ids[token_end_index] != 1:\n",
    "                token_end_index -= 1\n",
    "\n",
    "            word_ids = encoding.word_ids(i)[token_start_index : token_end_index + 1]\n",
    "            start_position = cls_index\n",
    "            end_position = cls_index\n",
    "\n",
    "            #loop over word_ids and increase token_start_index until it matches the answer position in words\n",
    "            #if it matches, save the token_start_index as the start_position of the answer in the encoding\n",
    "            for id in word_ids:\n",
    "                if id == word_idx_start:\n",
    "                    start_position = token_start_index\n",
    "                else:\n",
    "                    token_start_index += 1\n",
    "            \n",
    "\n",
    "            #loop over word_ids starting from the end to find the end_position of the answer\n",
    "            for id in word_ids[::-1]:\n",
    "                if id == word_idx_end:\n",
    "                    end_position = token_end_index\n",
    "                else:\n",
    "                    token_end_index -= 1\n",
    "\n",
    "            print(\"True answer:\", answer)\n",
    "            start_positions.append(start_position)\n",
    "            end_positions.append(end_position)\n",
    "            reconstructed_answer = tokenizer.decode(encoding.input_ids[cls_index][start_position:end_position+1])\n",
    "            print(\"Reconstructed answer:\", reconstructed_answer)\n",
    "\n",
    "        else:\n",
    "            start_positions.append(cls_index)\n",
    "            end_positions.append(cls_index)\n",
    "        \n",
    "        print(\"\\n\\n\\n\")\n",
    "\n",
    "    encoding[\"image\"] = examples[\"image\"]\n",
    "    encoding[\"start_positions\"] = start_positions\n",
    "    encoding[\"end_positions\"] = end_positions\n",
    "\n",
    "    \n",
    "\n",
    "    return encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "gather": {
     "logged": 1691402714325
    }
   },
   "outputs": [],
   "source": [
    "#define custom features\n",
    "features = Features({\n",
    "    'input_ids': Sequence(feature=Value(dtype='int64')),\n",
    "    'bbox': Array2D(dtype=\"int64\", shape=(512, 4)),\n",
    "    'attention_mask': Sequence(Value(dtype='int64')),\n",
    "    'token_type_ids': Sequence(Value(dtype='int64')),\n",
    "    'image': Array3D(dtype=\"int64\", shape=(3, 224, 224)),\n",
    "    'start_positions': Value(dtype='int64'),\n",
    "    'end_positions': Value(dtype='int64'),\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-06T14:06:52.312531Z",
     "start_time": "2023-07-06T14:05:27.824054Z"
    },
    "gather": {
     "logged": 1691402717490
    }
   },
   "outputs": [],
   "source": [
    "#encode entire dataset\n",
    "encoded_train_dataset = dataset_with_ocr_train.map(\n",
    "    encode_dataset, batched=True, batch_size=2, remove_columns=dataset_with_ocr_train.column_names, features=features\n",
    ")\n",
    "encoded_test_dataset = dataset_with_ocr_test.map(\n",
    "    encode_dataset, batched=True, batch_size=2, remove_columns=dataset_with_ocr_test.column_names, features=features\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-06T14:06:56.884739Z",
     "start_time": "2023-07-06T14:06:56.868498Z"
    },
    "gather": {
     "logged": 1691402719037
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': Sequence(feature=Value(dtype='int64', id=None), length=-1, id=None),\n",
       " 'bbox': Array2D(shape=(512, 4), dtype='int64', id=None),\n",
       " 'attention_mask': Sequence(feature=Value(dtype='int64', id=None), length=-1, id=None),\n",
       " 'token_type_ids': Sequence(feature=Value(dtype='int64', id=None), length=-1, id=None),\n",
       " 'image': Array3D(shape=(3, 224, 224), dtype='int64', id=None),\n",
       " 'start_positions': Value(dtype='int64', id=None),\n",
       " 'end_positions': Value(dtype='int64', id=None)}"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded_train_dataset.features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "gather": {
     "logged": 1691402719446
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_ids torch.Size([4, 512])\n",
      "bbox torch.Size([4, 512, 4])\n",
      "attention_mask torch.Size([4, 512])\n",
      "token_type_ids torch.Size([4, 512])\n",
      "image torch.Size([4, 3, 224, 224])\n",
      "start_positions torch.Size([4])\n",
      "end_positions torch.Size([4])\n"
     ]
    }
   ],
   "source": [
    "encoded_train_dataset.set_format(type=\"torch\")\n",
    "train_dataloader = torch.utils.data.DataLoader(encoded_train_dataset, batch_size=4)\n",
    "batch = next(iter(train_dataloader))\n",
    "\n",
    "for k,v in batch.items():\n",
    "    print(k, v.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "gather": {
     "logged": 1691402719907
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_ids torch.Size([4, 512])\n",
      "bbox torch.Size([4, 512, 4])\n",
      "attention_mask torch.Size([4, 512])\n",
      "token_type_ids torch.Size([4, 512])\n",
      "image torch.Size([4, 3, 224, 224])\n",
      "start_positions torch.Size([4])\n",
      "end_positions torch.Size([4])\n"
     ]
    }
   ],
   "source": [
    "encoded_test_dataset.set_format(type=\"torch\")\n",
    "eval_dataloader = torch.utils.data.DataLoader(encoded_test_dataset, batch_size=4)\n",
    "batch = next(iter(eval_dataloader))\n",
    "\n",
    "for k,v in batch.items():\n",
    "    print(k, v.shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/wr/mtgntxb97pq3k49fsgctpqs00000gp/T/ipykernel_62830/1691908840.py:13: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library ðŸ¤— Evaluate: https://huggingface.co/docs/evaluate\n",
      "  metric = load_metric('accuracy')\n"
     ]
    }
   ],
   "source": [
    "metric = load_metric('accuracy')\n",
    "metric1 = load_metric('accuracy')\n",
    "\n",
    "\n",
    "root_dir = '/Users/svenja/Downloads/dataset/'\n",
    "model_acc = 0\n",
    "ANLS_final_score = 0\n",
    "model_acc_divider = len(dataset[\"test\"])\n",
    "\n",
    "anls_list = []\n",
    "accuracy_list = []\n",
    "recall_list = []\n",
    "precision_list = []\n",
    "f1_list = []\n",
    "\n",
    "def ANLS(pred,ans):\n",
    "    if ans[0] is not None:\n",
    "        scores = []\n",
    "        ed = editdistance.eval(ans.lower(),pred.lower())\n",
    "        NL = ed/max(len(ans),len(pred))\n",
    "        scores.append(1-NL if NL<0.5 else 0)\n",
    "        return [max(scores)]\n",
    "    return []\n",
    "\n",
    "def compute_metrics(eval_preds):\n",
    "    anls_epoch = 0\n",
    "    accuracy_epoch = 0\n",
    "    recall_epoch = 0\n",
    "    precision_epoch = 0\n",
    "    f1_epoch = 0\n",
    "    \n",
    "        \n",
    "    for example in dataset[\"test\"]:\n",
    "        epoch = example['id']\n",
    "        question = example['query']\n",
    "        image_path = root_dir + '/testing_data/images/' + example['image']\n",
    "        image = Image.open(image_path).convert(\"RGB\")\n",
    "        answer = example['answer']\n",
    "\n",
    "        print(\"Question:\", question,\"\\n\")\n",
    "\n",
    "        processor = LayoutLMv2Processor.from_pretrained(model_checkpoint)\n",
    "\n",
    "        #prepare for the model\n",
    "        encoding = processor(image, question, return_tensors=\"pt\")\n",
    "\n",
    "        #forward pass\n",
    "        for k,v in encoding.items():\n",
    "            encoding[k] = v.to(model.device)\n",
    "\n",
    "        outputs = model(**encoding)\n",
    "\n",
    "\n",
    "        #get start_logits and end_logits\n",
    "        start_logits = outputs.start_logits\n",
    "        end_logits = outputs.end_logits\n",
    "\n",
    "        #get largest logit for both\n",
    "        predicted_start_idx = start_logits.argmax(-1).item()\n",
    "        predicted_end_idx = end_logits.argmax(-1).item()\n",
    "        \n",
    "        #decode the predicted answer\n",
    "        predicted_answer = processor.tokenizer.decode(encoding.input_ids.squeeze()[predicted_start_idx:predicted_end_idx+1])\n",
    "        \n",
    "        pred_tensor = encoding.input_ids.squeeze()[predicted_start_idx:predicted_end_idx+1]\n",
    "\n",
    "\n",
    "        print(\"True answer:\", answer)\n",
    "        print(\"Predicted answer:\", predicted_answer, \"\\n\")\n",
    "\n",
    "        # Part for ANLS\n",
    "        scores=[]\n",
    "        scores+=ANLS(predicted_answer, answer)\n",
    "        final_score = np.mean(scores)\n",
    "        print('ID: {}, ANLS: {}'.format(epoch,final_score))\n",
    "        torch.save({'state_dict':model.state_dict(), 'epoch':epoch, 'ANLS':final_score},'state_latest.pth')\n",
    "\n",
    "        answer_tensor = torch.LongTensor(list(bytes(answer, 'utf8')))\n",
    "        pred_tensor = torch.LongTensor(list(bytes(predicted_answer, 'utf8')))\n",
    "\n",
    "        max_length = max(answer_tensor.size(0), pred_tensor.size(0))\n",
    "        answer_tensor = torch.cat([answer_tensor, torch.zeros(max_length - answer_tensor.size(0))], dim=0)\n",
    "        pred_tensor = torch.cat([pred_tensor, torch.zeros(max_length - pred_tensor.size(0))], dim=0)\n",
    "\n",
    "        accuracy = 0\n",
    "        recall = 0\n",
    "        precision = 0\n",
    "        f1 = 0\n",
    "        accuracy = accuracy_score(answer_tensor, pred_tensor, normalize=True, sample_weight=None)\n",
    "        recall = recall_score(answer_tensor, pred_tensor, average='weighted', zero_division=0)\n",
    "        precision = precision_score(answer_tensor, pred_tensor, average='weighted', zero_division=0)\n",
    "        f1 = f1_score(answer_tensor, pred_tensor, average='weighted')\n",
    "        \n",
    "        print(\"accuracy:\", accuracy)\n",
    "        print(\"recall:\", recall)\n",
    "        print(\"precision:\", precision)\n",
    "        print(\"f1:\", f1)\n",
    "        \n",
    "        anls_epoch += final_score\n",
    "        accuracy_epoch += accuracy\n",
    "        recall_epoch += recall\n",
    "        precision_epoch += precision\n",
    "        f1_epoch += f1\n",
    "        \n",
    "        \n",
    "        \n",
    "        print('')\n",
    "        print(\"--------------------------------------\")\n",
    "        \n",
    "    anls_epoch = anls_epoch/model_acc_divider\n",
    "    accuracy_epoch = accuracy_epoch/model_acc_divider\n",
    "    recall_epoch = recall_epoch/model_acc_divider\n",
    "    precision_epoch = precision_epoch/model_acc_divider\n",
    "    f1_epoch = f1_epoch/model_acc_divider\n",
    "    \n",
    "\n",
    "    epoch_values = dict()\n",
    "    epoch_values['ANLS'] = anls_epoch\n",
    "    epoch_values['Accuracy'] = accuracy_epoch\n",
    "    epoch_values['Recall'] = recall_epoch\n",
    "    epoch_values['Precision'] = precision_epoch\n",
    "    epoch_values['F1'] = f1_epoch\n",
    "    \n",
    "    anls_list.append(anls_epoch)\n",
    "    accuracy_list.append(accuracy_epoch)\n",
    "    recall_list.append(recall_epoch)\n",
    "    precision_list.append(precision_epoch)\n",
    "    f1_list.append(f1_epoch)\n",
    "    \n",
    "    print(\"ANLS\", anls_list)\n",
    "    print(\"Accuracy\", accuracy_list)\n",
    "    print(\"Recall\", recall_list)\n",
    "    print(\"Precision\", precision_list)\n",
    "    print(\"F1\", f1_list)\n",
    "    \n",
    "    print(\"Epoch values:\", epoch_values)\n",
    "    \n",
    "    print('+++++++++++++++++++++++++++++++++++++++++++')\n",
    "    return epoch_values\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-06T14:07:08.663428Z",
     "start_time": "2023-07-06T14:07:03.863892Z"
    },
    "gather": {
     "logged": 1691402733935
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/layoutlmv2-base-uncased were not used when initializing LayoutLMv2ForQuestionAnswering: ['layoutlmv2.visual.backbone.bottom_up.res4.13.conv2.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.9.conv1.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res5.2.conv2.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.15.conv1.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.5.conv2.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.6.conv3.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.15.conv2.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.20.conv2.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res2.0.conv3.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.stem.conv1.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.20.conv3.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.11.conv1.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.13.conv1.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res3.1.conv3.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res3.0.conv1.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.6.conv2.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.20.conv1.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res2.2.conv2.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.2.conv1.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.9.conv2.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res2.2.conv3.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res3.3.conv3.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.3.conv1.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.3.conv3.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res3.1.conv2.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res5.1.conv1.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.17.conv2.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.16.conv2.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.17.conv1.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res3.0.conv2.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.17.conv3.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.8.conv3.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.15.conv3.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.0.conv1.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.19.conv1.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.22.conv3.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res5.1.conv2.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.10.conv1.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res3.3.conv1.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res3.3.conv2.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res5.0.conv3.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res2.1.conv3.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res2.1.conv2.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.21.conv2.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.19.conv3.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.7.conv3.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res3.1.conv1.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.7.conv2.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res2.1.conv1.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.21.conv3.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.18.conv2.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res2.2.conv1.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.4.conv2.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.7.conv1.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.22.conv1.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res5.2.conv1.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.8.conv1.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res2.0.shortcut.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res5.0.shortcut.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.11.conv3.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res3.2.conv3.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.11.conv2.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.14.conv1.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.16.conv1.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.18.conv1.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.13.conv3.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.12.conv3.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.1.conv3.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.0.shortcut.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.18.conv3.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.19.conv2.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.5.conv1.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res5.0.conv2.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.6.conv1.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.12.conv2.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.5.conv3.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.0.conv2.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res3.0.shortcut.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.16.conv3.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res3.0.conv3.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.22.conv2.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res5.1.conv3.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.12.conv1.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.9.conv3.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.21.conv1.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res3.2.conv1.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.0.conv3.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.4.conv3.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.1.conv1.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.4.conv1.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.14.conv2.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res5.0.conv1.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.2.conv2.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.8.conv2.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.14.conv3.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.10.conv3.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res5.2.conv3.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res2.0.conv2.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res3.2.conv2.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.3.conv2.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.1.conv2.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.2.conv3.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res2.0.conv1.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.10.conv2.norm.num_batches_tracked']\n",
      "- This IS expected if you are initializing LayoutLMv2ForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing LayoutLMv2ForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of LayoutLMv2ForQuestionAnswering were not initialized from the model checkpoint at microsoft/layoutlmv2-base-uncased and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight', 'layoutlmv2.visual_segment_embedding']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = AutoModelForDocumentQuestionAnswering.from_pretrained(model_checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-06T14:13:51.157786Z",
     "start_time": "2023-07-06T14:13:51.120497Z"
    },
    "gather": {
     "logged": 1691402735365
    }
   },
   "outputs": [],
   "source": [
    "notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-15T11:42:57.934713Z",
     "start_time": "2023-07-15T11:42:57.921218Z"
    },
    "gather": {
     "logged": 1691402734684
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/lib/python3.11/site-packages/transformers/optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "repo_id = \"svenjars/layoutlmv2\"\n",
    "\n",
    "#define training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=repo_id,\n",
    "    per_device_train_batch_size=1,\n",
    "    num_train_epochs=20,\n",
    "    save_steps=20,\n",
    "    logging_steps=20, \n",
    "    eval_steps=20, \n",
    "    evaluation_strategy=\"epoch\",\n",
    "    learning_rate=5e-5,\n",
    "    save_total_limit=2,\n",
    "    remove_unused_columns=False,\n",
    "    push_to_hub=True,\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "gather": {
     "logged": 1691402735685
    }
   },
   "outputs": [],
   "source": [
    "#batch examples together\n",
    "data_collator = DefaultDataCollator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-15T11:41:59.085784Z",
     "start_time": "2023-07-15T11:41:50.109190Z"
    },
    "gather": {
     "logged": 1691402736013
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cloning https://huggingface.co/svenjars/layoutlmv2 into local empty directory.\n"
     ]
    }
   ],
   "source": [
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "#define trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    data_collator=data_collator,\n",
    "    train_dataset=encoded_train_dataset,\n",
    "    eval_dataset=encoded_test_dataset,\n",
    "    tokenizer=processor,\n",
    "    compute_metrics = compute_metrics,\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1691402736364
    }
   },
   "outputs": [],
   "source": [
    "#call trainer\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create model card and push to huggingface\n",
    "trainer.create_model_card()\n",
    "trainer.push_to_hub()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#evaluating the training\n",
    "trainer.evaluate()"
   ]
  }
 ],
 "metadata": {
  "kernel_info": {
   "name": "python3"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "microsoft": {
   "host": {
    "AzureML": {
     "notebookHasBeenCompleted": true
    }
   },
   "ms_spell_check": {
    "ms_spell_check_language": "en"
   }
  },
  "nteract": {
   "version": "nteract-front-end@1.0.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
